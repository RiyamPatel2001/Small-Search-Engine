HIGH-LEVEL PROGRAM DESIGN
--------------------------
The web crawler follows a multi-threaded breadth-first search approach with priority-based URL selection to ensure domain diversity. The program operates in the following phases:

1. INITIALIZATION
   - Creates a priority queue for URLs to be crawled
   - Initializes thread-safe data structures (visited URLs set, domain counters)
   - Sets up an HTTP session with connection pooling and retry logic
   - Creates a timestamped log file for output

2. SEED GENERATION
   - Queries DuckDuckGo search API with the provided search term
   - Retrieves the specified number of seed URLs from search results
   - Adds seed URLs to the priority queue with initial priority scores
   - Marks these URLs as seed URLs (they bypass robots.txt checks)

3. MULTI-THREADED CRAWLING
   - Spawns multiple worker threads that operate concurrently
   - Each thread continuously pulls URLs from the priority queue
   - For each URL:
     a) Check if it's already visited (skip if yes)
     b) Check robots.txt permissions (unless it's a seed URL)
     c) Download the page content via HTTP GET request
     d) Log crawl information (URL, timestamp, size, depth, status code)
     e) Extract all hyperlinks from HTML content
     f) Calculate priority scores for discovered links
     g) Add new unvisited links back to the priority queue
   - Threads coordinate via locks to safely update shared data structures

4. PRIORITY CALCULATION FOR DOMAIN DIVERSITY
   - Each URL is assigned a priority score based on how many pages from its domain have been crawled
   - Domains with fewer crawled pages receive higher priority (lower score)
   - This prevents the crawler from getting stuck on a single domain
   - Priority tiers:
     * Score 1: New domain (0 pages crawled)
     * Score 2: 1-2 pages crawled from domain
     * Score 3: 3-5 pages crawled from domain
     * Score 10: 6+ pages crawled from domain

5. TERMINATION AND STATISTICS
   - Crawling stops when max_pages is reached or queue is exhausted
   - All threads are signaled to stop via "STOP" sentinel values
   - Comprehensive statistics are computed and appended to the log file
   - Statistics include: total pages, time taken, speed, data size, status codes, depth distribution, domain distribution


KEY IMPLEMENTATION DETAILS
---------------------------
- Thread Safety: All shared data structures (visited URLs, domain counts, statistics) are protected by locks to prevent race conditions
- Robots.txt Caching: Each domain's robots.txt is fetched once and cached to avoid redundant requests
- URL Validation: Filters out non-HTTP protocols, binary files, and invalid URL formats
- Session Pooling: Uses requests.Session with connection pooling for efficiency
- Graceful Shutdown: Handles KeyboardInterrupt (Ctrl+C) and ensures statistics are saved


LIBRARIES AND RESOURCES USED
-----------------------------
Standard Python Libraries:
- argparse: Command-line argument parsing
- queue: Thread-safe priority queue implementation
- threading: Multi-threading support
- urllib: URL parsing and robots.txt handling
- datetime, time: Timestamps and timing measurements

Third-Party Libraries:
- requests: HTTP requests with session management and retry logic
- beautifulsoup4: HTML parsing and link extraction
- ddgs: DuckDuckGo search API for seed URL generation
- urllib3: Advanced HTTP connection pooling

KNOWN LIMITATIONS AND BUGS
---------------------------
None identified. The program functions as intended with the following behavioral notes:

- Seed URLs always bypass robots.txt restrictions (intentional design choice)
- Pages that redirect are logged with their final URL and final status code
- Network errors and timeouts are logged as "ERROR" status codes
- Very large pages (>10MB) may take longer to process but are handled correctly
- The crawler does not follow JavaScript-rendered links (only static HTML)
- For niche search terms or topics with many sites blocking crawlers via robots.txt, the queue may become empty before reaching max_pages. Increase seed_pages parameter to mitigate this.


SPECIAL FEATURES BEYOND BASIC REQUIREMENTS
-------------------------------------------
1. Robots.txt Caching
   - Caches robots.txt files per domain to minimize redundant requests
   - Includes timeout optimization (0.5 seconds) to handle unresponsive robots.txt servers
   - Significantly improves crawling efficiency for sites with many pages